# Distributional-Models-of-Semantics

In this project, I use the context-sensitive distributional models of Dinu and Lapata (2010). In particular, I use simple vector addition, simple vector multiplication, and LDA topic models for the lexical substitution task.
The data consists of the 20,000 most frequent words in the British National Corpus (BNC).

##########################################################################################

The answer for part C question 1 :

The similarity between 'house' and 'home' is : 0.812743856464 \n
The similarity between 'house' and 'time' is : 0.82359219053  \n
The similarity between 'home' and 'time' is : 0.818144793373 \n

The result is frustrating, This model doesn’t reflect the similarity meaning between ‘house’ and ‘home’.
